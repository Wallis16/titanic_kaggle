{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Ex_4.05.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"U21P1stQ0oNo"},"source":["#### *Exercise 4.5*\n","\n","#### How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."]},{"cell_type":"markdown","metadata":{"id":"TAr0M8ag0oNw"},"source":["---\n","#### Resposta 1:\n","\n","1. Inicialize $Q(s,a) ∈ ℝ$ e $π(s) ∈ A(s)$ arbitrariamente para todo $s ∈ S$, $a ∈ A$.\n","\n","2. Policy Evaluation \\\n","\n","Loop: \\\n","\\\n","$~~~~~~~$$Δ$ <-- $0$ \\\n","$~~~~~~~$Loop para cada $s ∈ S$ e $a ∈ A(s)$: \\\n","$~~~~~~~~~~~~~$$q$ <-- $Q(s,a)$ \\\n","$~~~~~~~~~~~~~$$Q(s,a)$ <-- $ \\sum_{s', r}{p(s',r|s,a)[r + γ\\sum_{a'}{π(a'|s')Q(s',a')} ]} $ \\\n","$~~~~~~~~~~~~~$$Δ$ <-- $max(Δ,|q - Q(s,a)|)$ \\\n","\\\n","até $Δ < θ$ (um pequeno número positivo determinando a acurácia da estimação) \\\n","\n","3. Policy improvement\n","\n","policy-stable <-- true\n","\n","Para cada $s ∈ S$: \\\n","$~~~~~~~$old-action <-- $π(s)$ \\\n","$~~~~~~~$$π(s)$ <-- $argmax  Q(s,a)$ \\\n","$~~~~~~~$Se old-action ≠ $π(s)$, então plicy-stable <-- false \\\n","\\\n","Se policy-stable for true, então pare e retorne $Q$ ≃ $q_*$ e $π$ ≃ $π_*$;\n","\n","Do contrário, vá para 2.\n","\n","A expressão $ \\sum_{s', r}{p(s',r|s,a)[r + γ\\sum_{a'}{π(a'|s')Q(s',a')} ]} $ foi obtida reescrevendo a equação de Bellman (4.2). Semelhante a (4.4) em relação a (4.1), que ao invés de calcular o valor máximo, substitui por $ \\sum_{a}{π(a,s)}$. No caso considerado, ao invés de calcular o valor máximo da função ação valor, calcula-se o $ \\sum_{a}{π(a|s)}$. A partir daí calcula-se $q$ iterativamente, semelhante a $v_{k+1}$ em (4.5)."]}]}